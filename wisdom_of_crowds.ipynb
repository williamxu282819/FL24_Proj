{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from architecture import *\n",
    "train_images, train_labels, test_images, test_labels, val_images, val_labels = load_n_images(2400)\n",
    "# make train_labels and test_labels binary: 0-4 -> 0, 5-9 -> 1\n",
    "# train_labels = np.array([0 if x < 5 else 1 for x in train_labels])\n",
    "# test_labels = np.array([0 if x < 5 else 1 for x in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_cnn_run(args, seed, train_images, train_labels, test_images, test_labels, val_images, val_labels):\n",
    "    # Set seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_dataset = TensorDataset(torch.Tensor(train_images).unsqueeze(1), torch.Tensor(train_labels).long())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(torch.Tensor(val_images).unsqueeze(1), torch.Tensor(val_labels).long())\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(torch.Tensor(test_images).unsqueeze(1), torch.Tensor(test_labels).long())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate models\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = Encoder(args).to(device)\n",
    "    classifier = Class_out(args).to(device)\n",
    "    conf_out = Conf_out(args).to(device)\n",
    "\n",
    "    # Define loss functions and optimizer\n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_conf = nn.BCELoss()  # Binary Cross Entropy for confidence prediction\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()) + list(conf_out.parameters()), lr=args.learning_rate)\n",
    "\n",
    "    # Run the CNN denoise training loop\n",
    "    best_model, best_z, best_conv_flat, test_z, test_conv_flat, stats = CNN_denoise(\n",
    "        encoder=encoder,\n",
    "        classifier=classifier,\n",
    "        conf_out=conf_out,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion_class=criterion_class,\n",
    "        criterion_conf=criterion_conf,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Return or log the results\n",
    "    results = {\n",
    "        #'best_model': best_model,        # Contains state_dicts of encoder, classifier, conf_out\n",
    "        'best_train_z': best_z.numpy(),         # Latent vectors for training set\n",
    "        'best_train_conv_flat': best_conv_flat.numpy(),  # Flattened conv layer output for training set\n",
    "        'best_test_z': test_z.numpy(),           # Latent vectors for test set\n",
    "        'best_test_conv_flat': test_conv_flat.numpy(),   # Flattened conv layer output for test set\n",
    "        'stats': stats                  # Training statistics\n",
    "    }\n",
    "\n",
    "    one_cnn_stats = stats\n",
    "    print(one_cnn_stats)\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_acc': 0.9004, 'train_conf': 0.8878, 'test_acc': 0.8703, 'test_conf': 0.8841, 'best_loss': 0.7646, 'best_acc': 0.8333, 'best_conf': 0.7757}\n",
      "{'train_acc': 0.8535, 'train_conf': 0.9395, 'test_acc': 0.8234, 'test_conf': 0.9536, 'best_loss': 0.739, 'best_acc': 0.875, 'best_conf': 0.783}\n",
      "{'train_acc': 0.7852, 'train_conf': 0.9562, 'test_acc': 0.7932, 'test_conf': 0.9559, 'best_loss': 0.6096, 'best_acc': 0.9062, 'best_conf': 0.8345}\n",
      "{'train_acc': 0.7852, 'train_conf': 0.9468, 'test_acc': 0.7203, 'test_conf': 0.9463, 'best_loss': 0.5737, 'best_acc': 0.9219, 'best_conf': 0.8312}\n",
      "{'train_acc': 0.6641, 'train_conf': 0.9616, 'test_acc': 0.651, 'test_conf': 0.9605, 'best_loss': 0.7365, 'best_acc': 0.8906, 'best_conf': 0.7772}\n",
      "{'train_acc': 0.7559, 'train_conf': 0.9463, 'test_acc': 0.7125, 'test_conf': 0.9483, 'best_loss': 0.5924, 'best_acc': 0.9219, 'best_conf': 0.8852}\n",
      "{'train_acc': 0.8828, 'train_conf': 0.951, 'test_acc': 0.8521, 'test_conf': 0.9493, 'best_loss': 0.4269, 'best_acc': 0.9531, 'best_conf': 0.8762}\n",
      "{'train_acc': 0.8105, 'train_conf': 0.9121, 'test_acc': 0.851, 'test_conf': 0.9117, 'best_loss': 0.7188, 'best_acc': 0.875, 'best_conf': 0.8382}\n",
      "{'train_acc': 0.748, 'train_conf': 0.959, 'test_acc': 0.7146, 'test_conf': 0.9593, 'best_loss': 0.7846, 'best_acc': 0.9062, 'best_conf': 0.7316}\n",
      "{'train_acc': 0.6211, 'train_conf': 0.9108, 'test_acc': 0.6495, 'test_conf': 0.9187, 'best_loss': 0.6731, 'best_acc': 0.875, 'best_conf': 0.862}\n"
     ]
    }
   ],
   "source": [
    "train_zs = []\n",
    "train_conv_flats = []\n",
    "test_zs = []\n",
    "test_conv_flats = []\n",
    "loop_num = 10\n",
    "\n",
    "# Hyperparameters\n",
    "class Args:\n",
    "    latent_dim = 64 # 10, 32, 64, 128, 256... this would cause PCA testing accuracy to go up\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "    learning_rate = 0.01\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Run the CNN denoise training loop\n",
    "for seed in range(loop_num):\n",
    "    results = one_cnn_run(args, seed, train_images, train_labels, test_images, test_labels, val_images, val_labels)\n",
    "    train_zs.append(results['best_train_z'])\n",
    "    train_conv_flats.append(results['best_train_conv_flat'])\n",
    "    test_zs.append(results['best_test_z'])\n",
    "    test_conv_flats.append(results['best_test_conv_flat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using PCA: 87.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_pca, test_pca, train_reconstructed, test_reconstructed = PCA_reduction(train_images, test_images, args.latent_dim, random_seed=0)\n",
    "# Logistic regression classifier for PCA-based latent space\n",
    "clf_pca = LogisticRegression(max_iter=loop_num)\n",
    "clf_pca.fit(train_pca, train_labels)  # Train on PCA latent space\n",
    "test_preds_pca = clf_pca.predict(test_pca)  # Predict on test set\n",
    "# Compute accuracy\n",
    "acc_pca = accuracy_score(test_labels, test_preds_pca)\n",
    "print(f\"Accuracy on test set using PCA: {acc_pca * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.1767\n",
      "Average confidence of predictions: 0.1321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "\n",
    "def vote_majority(X):\n",
    "    # along the second axis, find the most common element\n",
    "    return np.array([Counter(column).most_common(1)[0][0] for column in X.T])\n",
    "\n",
    "# Initialize a list to store the decision trees\n",
    "decision_trees = []\n",
    "prediction_trees = []\n",
    "prediction_trees_probs = []\n",
    "\n",
    "for i in range(loop_num): \n",
    "    # # Create a new decision tree\n",
    "    tree = DecisionTreeClassifier(max_depth=3, random_state=i, max_features=8) # change max_features according to latent_dim\n",
    "    # tree = RandomForestClassifier(n_estimators=loop_num, max_depth=5, random_state=i, max_features=64)\n",
    "    # Fit the decision tree on the latent representations\n",
    "    training_array = train_zs[i]\n",
    "    tree.fit(training_array, train_labels)\n",
    "    # Append the decision tree to the list\n",
    "    decision_trees.append(tree)\n",
    "\n",
    "    # Predict the test labels using the decision tree\n",
    "    testing_array = test_zs[i]\n",
    "    \n",
    "    prediction_tree = tree.predict(testing_array)\n",
    "    prediction_trees.append(prediction_tree)\n",
    "\n",
    "    # Predict the test labels using the decision tree and calculate the probabilities\n",
    "    prediction_tree_probs = tree.predict_proba(testing_array)\n",
    "    prediction_trees_probs.append(prediction_tree_probs)\n",
    "\n",
    "prediction_trees = np.array(prediction_trees)\n",
    "prediction_trees_probs = np.array(prediction_trees_probs)\n",
    "\n",
    "# vote for the majority\n",
    "rf_predictions = vote_majority(prediction_trees)\n",
    "# calculate the accuracy\n",
    "print(f\"Testing accuracy: {accuracy_score(test_labels, rf_predictions):.4f}\")\n",
    "# Calculate the average probability for each class across all trees\n",
    "average_probs = np.mean(prediction_trees_probs, axis=0)\n",
    "# Calculate the average confidence of the predictions\n",
    "average_confidences = np.max(average_probs, axis=1)\n",
    "average_confidence = np.mean(average_confidences)\n",
    "print(f\"Average confidence of predictions: {average_confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.1333\n",
      "Average confidence of predictions: 0.1234\n"
     ]
    }
   ],
   "source": [
    "# do the same analysis using XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize a list to store the XGBoost models\n",
    "xgboost_models = []\n",
    "prediction_xgboost = []\n",
    "prediction_xgboost_probs = []\n",
    "\n",
    "for i in range(loop_num):\n",
    "    # Create a new XGBoost model\n",
    "    xgboost = XGBClassifier(n_estimators=loop_num, random_state=i, learning_rate=0.1)\n",
    "    # Fit the XGBoost model on the latent representations\n",
    "    training_array = train_zs[i]\n",
    "    xgboost.fit(training_array, train_labels)\n",
    "    # Append the XGBoost model to the list\n",
    "    xgboost_models.append(xgboost)\n",
    "\n",
    "    # Predict the test labels using the XGBoost model\n",
    "    testing_array = test_zs[i]\n",
    "    prediction_xgb = xgboost.predict(testing_array)\n",
    "    prediction_xgboost.append(prediction_xgb)\n",
    "\n",
    "    # Predict the test labels using the XGBoost model and calculate the probabilities\n",
    "    prediction_xgb_probs = xgboost.predict_proba(testing_array)\n",
    "    prediction_xgboost_probs.append(prediction_xgb_probs)\n",
    "\n",
    "prediction_xgboost = np.array(prediction_xgboost)\n",
    "prediction_xgboost_probs = np.array(prediction_xgboost_probs)\n",
    "\n",
    "# vote for the majority\n",
    "xgboost_predictions = vote_majority(prediction_xgboost)\n",
    "# calculate the accuracy\n",
    "print(f\"Testing accuracy: {accuracy_score(test_labels, xgboost_predictions):.4f}\")\n",
    "# Calculate the average probability for each class across all trees\n",
    "average_probs = np.mean(prediction_xgboost_probs, axis=0)\n",
    "# Calculate the average confidence of the predictions\n",
    "average_confidences = np.max(average_probs, axis=1)\n",
    "average_confidence = np.mean(average_confidences)\n",
    "print(f\"Average confidence of predictions: {average_confidence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
