{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from binary_architecture import *\n",
    "train_images, train_labels, test_images, test_labels = load_n_images(1200)\n",
    "# make train_labels and test_labels binary: 0-4 -> 0, 5-9 -> 1\n",
    "train_labels = np.array([0 if x < 5 else 1 for x in train_labels])\n",
    "test_labels = np.array([0 if x < 5 else 1 for x in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_cnn_run(args, seed, train_images, train_labels, test_images, test_labels):\n",
    "    # Set seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_dataset = TensorDataset(torch.Tensor(train_images).unsqueeze(1), torch.Tensor(train_labels).long())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(torch.Tensor(test_images).unsqueeze(1), torch.Tensor(test_labels).long())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate models\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = Encoder(args).to(device)\n",
    "    classifier = Class_out(args).to(device)\n",
    "    conf_out = Conf_out(args).to(device)\n",
    "\n",
    "    # Define loss functions and optimizer\n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_conf = nn.BCELoss()  # Binary Cross Entropy for confidence prediction\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()) + list(conf_out.parameters()), lr=args.learning_rate)\n",
    "\n",
    "    # Run the CNN denoise training loop\n",
    "    best_model, test_z, test_conv_flat, stats = CNN_denoise(\n",
    "        encoder=encoder,\n",
    "        classifier=classifier,\n",
    "        conf_out=conf_out,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion_class=criterion_class,\n",
    "        criterion_conf=criterion_conf,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Return or log the results\n",
    "    results = {\n",
    "        'best_model': best_model,\n",
    "        'stats': stats\n",
    "    }\n",
    "\n",
    "    one_cnn_stats = stats\n",
    "    print(one_cnn_stats)\n",
    "    # Return the results\n",
    "    return results, test_z, test_conv_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': 0.8202, 'test_conf': 0.8176, 'best_loss': 0.5974, 'best_acc': 0.9219, 'best_conf': 0.9301}\n",
      "{'test_acc': 0.7236, 'test_conf': 0.9195, 'best_loss': 0.4452, 'best_acc': 0.9219, 'best_conf': 0.8804}\n",
      "{'test_acc': 0.8065, 'test_conf': 0.8856, 'best_loss': 0.7017, 'best_acc': 0.8594, 'best_conf': 0.9198}\n",
      "{'test_acc': 0.8588, 'test_conf': 0.9697, 'best_loss': 0.2697, 'best_acc': 1.0, 'best_conf': 0.8178}\n",
      "{'test_acc': 0.842, 'test_conf': 0.9729, 'best_loss': 0.4285, 'best_acc': 0.9219, 'best_conf': 0.9589}\n",
      "{'test_acc': 0.7577, 'test_conf': 0.8898, 'best_loss': 0.1932, 'best_acc': 1.0, 'best_conf': 0.9354}\n",
      "{'test_acc': 0.8173, 'test_conf': 0.9338, 'best_loss': 0.495, 'best_acc': 0.9375, 'best_conf': 0.8909}\n",
      "{'test_acc': 0.8142, 'test_conf': 0.9395, 'best_loss': 0.2688, 'best_acc': 1.0, 'best_conf': 0.8527}\n",
      "{'test_acc': 0.8588, 'test_conf': 0.9568, 'best_loss': 0.5287, 'best_acc': 0.9062, 'best_conf': 0.9693}\n",
      "{'test_acc': 0.8616, 'test_conf': 0.9133, 'best_loss': 0.2878, 'best_acc': 1.0, 'best_conf': 0.8475}\n"
     ]
    }
   ],
   "source": [
    "best_models = []\n",
    "loop_num = 10\n",
    "model_accs = []\n",
    "model_confs = []\n",
    "\n",
    "# Hyperparameters\n",
    "class Args:\n",
    "    latent_dim = 64 # 10, 32, 64, 128, 256... this would cause PCA testing accuracy to go up\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "    learning_rate = 0.01\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Run the CNN denoise training loop\n",
    "for seed in range(loop_num):\n",
    "    results, _, _ = one_cnn_run(args, seed, train_images, train_labels, test_images, test_labels)\n",
    "    best_models.append(results['best_model'])\n",
    "    model_accs.append(results['stats']['test_acc'])\n",
    "    model_confs.append(results['stats']['test_conf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def absolute_majority(models, test_loader, args, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    votes = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Initialize and move the model to the device\n",
    "    encoder = Encoder(args).to(device)\n",
    "    classifier = Class_out(args).to(device)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in test_loader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_votes = []\n",
    "            \n",
    "            # Collect votes from each model\n",
    "            for model in models:\n",
    "                # Load the weights into the models\n",
    "                encoder.load_state_dict(model['encoder'])  # Load encoder parameters\n",
    "                classifier.load_state_dict(model['classifier'])  # Load classifier parameters\n",
    "                \n",
    "                encoder.eval()\n",
    "                classifier.eval()\n",
    "\n",
    "                z, _ = encoder(batch_images, device)\n",
    "                class_preds = classifier(z)\n",
    "                batch_votes.append(torch.argmax(class_preds, dim=1).cpu().numpy())\n",
    "            \n",
    "            # Transpose to get votes per sample\n",
    "            batch_votes = np.array(batch_votes).T  # Shape (batch_size, num_models)\n",
    "            votes.extend(batch_votes)\n",
    "    \n",
    "    # Get the majority vote per sample\n",
    "    votes = np.array(votes)\n",
    "    majority_votes = [np.bincount(vote).argmax() for vote in votes]\n",
    "    \n",
    "    return majority_votes\n",
    "\n",
    "# def relative_majority(models, model_accs, test_loader, args, loop_num):\n",
    "#     votes = []\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     # Initialize and move the model to the device\n",
    "#     encoder = Encoder(args).to(device)\n",
    "#     classifier = Class_out(args).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch_images, batch_labels in test_loader:\n",
    "#             batch_images = batch_images.to(device)\n",
    "#             weighted_votes = np.zeros((batch_images.shape[0], loop_num))\n",
    "            \n",
    "#             # Collect weighted votes from each model\n",
    "#             for i, model in enumerate(models):\n",
    "#                  # Load the weights into the models\n",
    "#                 encoder.load_state_dict(model['encoder'])  # Load encoder parameters\n",
    "#                 classifier.load_state_dict(model['classifier'])  # Load classifier parameters\n",
    "                \n",
    "#                 encoder.eval()\n",
    "#                 classifier.eval()\n",
    "\n",
    "#                 z, _ = encoder(batch_images, device)\n",
    "#                 class_preds = classifier(z)\n",
    "                \n",
    "#                 # Accumulate weighted probabilities\n",
    "#                 weighted_votes += model_accs[i] * class_preds.cpu().numpy()\n",
    "                \n",
    "#             # Assign the class with the highest weighted vote\n",
    "#             votes.extend(np.argmax(weighted_votes, axis=1))\n",
    "    \n",
    "#     return votes\n",
    "\n",
    "def evaluate_ensemble(predictions, test_labels):\n",
    "    correct = (predictions == test_labels).sum()\n",
    "    total = len(test_labels)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "test_dataset = TensorDataset(torch.Tensor(test_images).unsqueeze(1), torch.Tensor(test_labels).long())\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute majority ensemble accuracy: 0.8066666666666666\n"
     ]
    }
   ],
   "source": [
    "abs_majority_preds = absolute_majority(best_models, test_loader, args)\n",
    "print(f'Absolute majority ensemble accuracy: {evaluate_ensemble(abs_majority_preds, test_labels)}')\n",
    "\n",
    "# rel_majority_preds = relative_majority(best_models, model_accs, test_loader, args, loop_num)\n",
    "# print(f'Relative majority ensemble accuracy: {evaluate_ensemble(rel_majority_preds, test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using PCA: 84.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_pca, test_pca, train_reconstructed, test_reconstructed = PCA_reduction(train_images, test_images, args.latent_dim, random_seed=0)\n",
    "# Logistic regression classifier for PCA-based latent space\n",
    "clf_pca = LogisticRegression(max_iter=loop_num)\n",
    "clf_pca.fit(train_pca, train_labels)  # Train on PCA latent space\n",
    "test_preds_pca = clf_pca.predict(test_pca)  # Predict on test set\n",
    "# Compute accuracy\n",
    "acc_pca = accuracy_score(test_labels, test_preds_pca)\n",
    "print(f\"Accuracy on test set using PCA: {acc_pca * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.1767\n",
      "Average confidence of predictions: 0.1321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "\n",
    "def vote_majority(X):\n",
    "    # along the second axis, find the most common element\n",
    "    return np.array([Counter(column).most_common(1)[0][0] for column in X.T])\n",
    "\n",
    "# Initialize a list to store the decision trees\n",
    "decision_trees = []\n",
    "prediction_trees = []\n",
    "prediction_trees_probs = []\n",
    "\n",
    "for i in range(loop_num): \n",
    "    # # Create a new decision tree\n",
    "    tree = DecisionTreeClassifier(max_depth=3, random_state=i, max_features=8) # change max_features according to latent_dim\n",
    "    # tree = RandomForestClassifier(n_estimators=loop_num, max_depth=5, random_state=i, max_features=64)\n",
    "    # Fit the decision tree on the latent representations\n",
    "    training_array = train_zs[i]\n",
    "    tree.fit(training_array, train_labels)\n",
    "    # Append the decision tree to the list\n",
    "    decision_trees.append(tree)\n",
    "\n",
    "    # Predict the test labels using the decision tree\n",
    "    testing_array = test_zs[i]\n",
    "    \n",
    "    prediction_tree = tree.predict(testing_array)\n",
    "    prediction_trees.append(prediction_tree)\n",
    "\n",
    "    # Predict the test labels using the decision tree and calculate the probabilities\n",
    "    prediction_tree_probs = tree.predict_proba(testing_array)\n",
    "    prediction_trees_probs.append(prediction_tree_probs)\n",
    "\n",
    "prediction_trees = np.array(prediction_trees)\n",
    "prediction_trees_probs = np.array(prediction_trees_probs)\n",
    "\n",
    "# vote for the majority\n",
    "rf_predictions = vote_majority(prediction_trees)\n",
    "# calculate the accuracy\n",
    "print(f\"Testing accuracy: {accuracy_score(test_labels, rf_predictions):.4f}\")\n",
    "# Calculate the average probability for each class across all trees\n",
    "average_probs = np.mean(prediction_trees_probs, axis=0)\n",
    "# Calculate the average confidence of the predictions\n",
    "average_confidences = np.max(average_probs, axis=1)\n",
    "average_confidence = np.mean(average_confidences)\n",
    "print(f\"Average confidence of predictions: {average_confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.1333\n",
      "Average confidence of predictions: 0.1234\n"
     ]
    }
   ],
   "source": [
    "# do the same analysis using XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize a list to store the XGBoost models\n",
    "xgboost_models = []\n",
    "prediction_xgboost = []\n",
    "prediction_xgboost_probs = []\n",
    "\n",
    "for i in range(loop_num):\n",
    "    # Create a new XGBoost model\n",
    "    xgboost = XGBClassifier(n_estimators=loop_num, random_state=i, learning_rate=0.1)\n",
    "    # Fit the XGBoost model on the latent representations\n",
    "    training_array = train_zs[i]\n",
    "    xgboost.fit(training_array, train_labels)\n",
    "    # Append the XGBoost model to the list\n",
    "    xgboost_models.append(xgboost)\n",
    "\n",
    "    # Predict the test labels using the XGBoost model\n",
    "    testing_array = test_zs[i]\n",
    "    prediction_xgb = xgboost.predict(testing_array)\n",
    "    prediction_xgboost.append(prediction_xgb)\n",
    "\n",
    "    # Predict the test labels using the XGBoost model and calculate the probabilities\n",
    "    prediction_xgb_probs = xgboost.predict_proba(testing_array)\n",
    "    prediction_xgboost_probs.append(prediction_xgb_probs)\n",
    "\n",
    "prediction_xgboost = np.array(prediction_xgboost)\n",
    "prediction_xgboost_probs = np.array(prediction_xgboost_probs)\n",
    "\n",
    "# vote for the majority\n",
    "xgboost_predictions = vote_majority(prediction_xgboost)\n",
    "# calculate the accuracy\n",
    "print(f\"Testing accuracy: {accuracy_score(test_labels, xgboost_predictions):.4f}\")\n",
    "# Calculate the average probability for each class across all trees\n",
    "average_probs = np.mean(prediction_xgboost_probs, axis=0)\n",
    "# Calculate the average confidence of the predictions\n",
    "average_confidences = np.max(average_probs, axis=1)\n",
    "average_confidence = np.mean(average_confidences)\n",
    "print(f\"Average confidence of predictions: {average_confidence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
