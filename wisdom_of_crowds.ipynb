{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "# from binary_architecture import *\n",
    "from multiclass_architecture import *\n",
    "from train_and_test import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(image_random_state=0, binary=True):\n",
    "    train_images, train_labels, test_images, test_labels = load_n_images(1200, random_state=image_random_state)\n",
    "    if binary:\n",
    "        train_labels = np.array([0 if x < 5 else 1 for x in train_labels])\n",
    "        test_labels = np.array([0 if x < 5 else 1 for x in test_labels])\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "def one_cnn_run(args, seed, train_images, train_labels, test_images, test_labels):\n",
    "    # Set seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_dataset = TensorDataset(torch.Tensor(train_images).unsqueeze(1), torch.Tensor(train_labels).long())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(torch.Tensor(test_images).unsqueeze(1), torch.Tensor(test_labels).long())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate models\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = Encoder(args).to(device)\n",
    "    classifier = Class_out(args).to(device)\n",
    "    conf_out = Conf_out(args).to(device)\n",
    "\n",
    "    # Define loss functions and optimizer\n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_conf = nn.BCELoss()  # Binary Cross Entropy for confidence prediction\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()) + list(conf_out.parameters()), lr=args.learning_rate)\n",
    "\n",
    "    # Run the CNN denoise training loop\n",
    "    best_model, test_z, test_conv_flat, stats = CNN_denoise(\n",
    "        encoder=encoder,\n",
    "        classifier=classifier,\n",
    "        conf_out=conf_out,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion_class=criterion_class,\n",
    "        criterion_conf=criterion_conf,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Return or log the results\n",
    "    results = {\n",
    "        'best_model': best_model,\n",
    "        'stats': stats\n",
    "    }\n",
    "\n",
    "    # one_cnn_stats = stats\n",
    "    # print(one_cnn_stats)\n",
    "\n",
    "    # Return the results\n",
    "    return results, test_z, test_conv_flat\n",
    "\n",
    "# Hyperparameters\n",
    "class Args:\n",
    "    latent_dim = 100 # this would cause PCA testing accuracy to go up\n",
    "    train_batch_size = 32\n",
    "    test_batch_size = 100\n",
    "    epochs = 5\n",
    "    learning_rate = 0.001\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def one_crowd(train_images, train_labels, test_images, test_labels, loop_num=10, args=args):\n",
    "    # Get the images\n",
    "    best_models = []\n",
    "    model_accs = []\n",
    "    model_confs = []\n",
    "\n",
    "    # Run the CNN denoise training loop\n",
    "    for seed in range(loop_num):\n",
    "        results, _, _ = one_cnn_run(args, seed, train_images, train_labels, test_images, test_labels)\n",
    "        best_models.append(results['best_model'])\n",
    "        model_accs.append(results['stats']['test_acc'])\n",
    "        model_confs.append(results['stats']['test_conf'])\n",
    "    return best_models, model_accs, model_confs\n",
    "\n",
    "def crowd_stats(loop_num=10,crowd_num=10,binary=True):\n",
    "    # use many image random states to generate many crowds and collect their stats\n",
    "    crowd_accs = []\n",
    "    crowd_confs = []\n",
    "    crowd_acc_means = []\n",
    "    crowd_conf_means = []\n",
    "    for i in range(crowd_num):\n",
    "        train_images, train_labels, test_images, test_labels = get_images(i+42, binary=binary)\n",
    "        _, model_accs, model_confs = one_crowd(train_images, train_labels, test_images, test_labels, loop_num)\n",
    "        crowd_accs.append(model_accs)\n",
    "        crowd_confs.append(model_confs)\n",
    "        crowd_acc_means.append(np.mean(model_accs))\n",
    "        crowd_conf_means.append(np.mean(model_confs))\n",
    "        print(f'crowd {i} done')\n",
    "    return crowd_accs, crowd_confs, crowd_acc_means, crowd_conf_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_accs, crowd_confs, crowd_acc_means, crowd_conf_means = crowd_stats(10,50,binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looking at Crowds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy:  0.8481458000000001\n",
      "Average test confidence:  0.8350041999999999\n",
      "Test accuracy std:  0.012781486469108374\n",
      "Test confidence std:  0.006225838125104118\n",
      "Test accuracy range:  0.87766 0.82\n",
      "Test confidence range:  0.8457800000000001 0.82007\n"
     ]
    }
   ],
   "source": [
    "# get the average, std, min and max of the averages\n",
    "print(\"Average test accuracy: \", np.mean(crowd_acc_means))\n",
    "print(\"Average test confidence: \", np.mean(crowd_conf_means))\n",
    "# stds\n",
    "print(\"Test accuracy std: \", np.std(crowd_acc_means))\n",
    "print(\"Test confidence std: \", np.std(crowd_conf_means))\n",
    "\n",
    "# range\n",
    "print(\"Test accuracy range: \", np.max(crowd_acc_means), np.min(crowd_acc_means))\n",
    "print(\"Test confidence range: \", np.max(crowd_conf_means), np.min(crowd_conf_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability histogram of distribution of all individual network accuracies\n",
    "plt.hist(np.array(crowd_accs).flatten(), bins=50, color='darkblue', alpha=0.7)\n",
    "plt.title('Distribution of individual network accuracies')\n",
    "# add a x=0.78 line \"PCA benchmark\"\n",
    "plt.axvline(x=0.82, color='black', linestyle='--', label='PCA benchmark')\n",
    "# add another line \"Crowd Average\"\n",
    "plt.axvline(x=np.mean(crowd_acc_means), color='blue', linestyle='--', label='Crowd Average')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('crowd_accs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability histogram of distribution of all individual network confidences\n",
    "plt.hist(np.array(crowd_confs).flatten(), bins=50, color='darkblue', alpha=0.7)\n",
    "plt.title('Distribution of individual network confidences')\n",
    "# add another line \"Crowd Average\"\n",
    "plt.axvline(x=np.mean(crowd_conf_means), color='blue', linestyle='--', label='Crowd Average')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('crowd_confs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def absolute_majority(models, test_loader, args, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    votes = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Initialize and move the model to the device\n",
    "    encoder = Encoder(args).to(device)\n",
    "    classifier = Class_out(args).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in test_loader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_votes = []\n",
    "            \n",
    "            # Collect votes from each model\n",
    "            for model in models:\n",
    "                # Load the weights into the models\n",
    "                encoder.load_state_dict(model['encoder'])  # Load encoder parameters\n",
    "                classifier.load_state_dict(model['classifier'])  # Load classifier parameters\n",
    "                \n",
    "                encoder.eval()\n",
    "                classifier.eval()\n",
    "\n",
    "                z, _ = encoder(batch_images, device)\n",
    "                class_preds = classifier(z)\n",
    "                batch_votes.append(torch.argmax(class_preds, dim=1).cpu().numpy())\n",
    "            \n",
    "            # Transpose to get votes per sample\n",
    "            batch_votes = np.array(batch_votes).T  # Shape (batch_size, num_models)\n",
    "            votes.extend(batch_votes)\n",
    "    \n",
    "    # Get the majority vote per sample\n",
    "    votes = np.array(votes)\n",
    "    majority_votes = [np.bincount(vote).argmax() for vote in votes]\n",
    "    \n",
    "    return majority_votes\n",
    "\n",
    "# def relative_majority(models, model_accs, test_loader, args, loop_num):\n",
    "#     votes = []\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     # Initialize and move the model to the device\n",
    "#     encoder = Encoder(args).to(device)\n",
    "#     classifier = Class_out(args).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch_images, batch_labels in test_loader:\n",
    "#             batch_images = batch_images.to(device)\n",
    "#             weighted_votes = np.zeros((batch_images.shape[0], loop_num))\n",
    "            \n",
    "#             # Collect weighted votes from each model\n",
    "#             for i, model in enumerate(models):\n",
    "#                  # Load the weights into the models\n",
    "#                 encoder.load_state_dict(model['encoder'])  # Load encoder parameters\n",
    "#                 classifier.load_state_dict(model['classifier'])  # Load classifier parameters\n",
    "                \n",
    "#                 encoder.eval()\n",
    "#                 classifier.eval()\n",
    "\n",
    "#                 z, _ = encoder(batch_images, device)\n",
    "#                 class_preds = classifier(z)\n",
    "                \n",
    "#                 # Accumulate weighted probabilities\n",
    "#                 weighted_votes += model_accs[i] * class_preds.cpu().numpy()\n",
    "                \n",
    "#             # Assign the class with the highest weighted vote\n",
    "#             votes.extend(np.argmax(weighted_votes, axis=1))\n",
    "    \n",
    "#     return votes\n",
    "\n",
    "def evaluate_ensemble(predictions, test_labels):\n",
    "    correct = (predictions == test_labels).sum()\n",
    "    total = len(test_labels)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "_, _, test_images, test_labels = get_images()\n",
    "test_dataset = TensorDataset(torch.Tensor(test_images).unsqueeze(1), torch.Tensor(test_labels).long())\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute majority ensemble accuracy: 0.8066666666666666\n"
     ]
    }
   ],
   "source": [
    "abs_majority_preds = absolute_majority(best_models, test_loader, args)\n",
    "print(f'Absolute majority ensemble accuracy: {evaluate_ensemble(abs_majority_preds, test_labels)}')\n",
    "\n",
    "# rel_majority_preds = relative_majority(best_models, model_accs, test_loader, args, loop_num)\n",
    "# print(f'Relative majority ensemble accuracy: {evaluate_ensemble(rel_majority_preds, test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set using PCA: 81.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# PCA-based latent space\n",
    "train_images, train_labels, test_images, test_labels = get_images(42)\n",
    "loop_num = 10\n",
    "\n",
    "train_pca, test_pca, train_reconstructed, test_reconstructed = PCA_reduction(train_images, test_images, args.latent_dim, random_seed=0)\n",
    "# Logistic regression classifier for PCA-based latent space\n",
    "clf_pca = LogisticRegression(max_iter=loop_num)\n",
    "clf_pca.fit(train_pca, train_labels)  # Train on PCA latent space\n",
    "test_preds_pca = clf_pca.predict(test_pca)  # Predict on test set\n",
    "# Compute accuracy\n",
    "acc_pca = accuracy_score(test_labels, test_preds_pca)\n",
    "print(f\"Accuracy on test set using PCA: {acc_pca * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.1767\n",
      "Average confidence of predictions: 0.1321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "\n",
    "def vote_majority(X):\n",
    "    # along the second axis, find the most common element\n",
    "    return np.array([Counter(column).most_common(1)[0][0] for column in X.T])\n",
    "\n",
    "# Initialize a list to store the decision trees\n",
    "decision_trees = []\n",
    "prediction_trees = []\n",
    "prediction_trees_probs = []\n",
    "\n",
    "for i in range(loop_num): \n",
    "    # # Create a new decision tree\n",
    "    tree = DecisionTreeClassifier(max_depth=3, random_state=i, max_features=8) # change max_features according to latent_dim\n",
    "    # tree = RandomForestClassifier(n_estimators=loop_num, max_depth=5, random_state=i, max_features=64)\n",
    "    # Fit the decision tree on the latent representations\n",
    "    training_array = train_zs[i]\n",
    "    tree.fit(training_array, train_labels)\n",
    "    # Append the decision tree to the list\n",
    "    decision_trees.append(tree)\n",
    "\n",
    "    # Predict the test labels using the decision tree\n",
    "    testing_array = test_zs[i]\n",
    "    \n",
    "    prediction_tree = tree.predict(testing_array)\n",
    "    prediction_trees.append(prediction_tree)\n",
    "\n",
    "    # Predict the test labels using the decision tree and calculate the probabilities\n",
    "    prediction_tree_probs = tree.predict_proba(testing_array)\n",
    "    prediction_trees_probs.append(prediction_tree_probs)\n",
    "\n",
    "prediction_trees = np.array(prediction_trees)\n",
    "prediction_trees_probs = np.array(prediction_trees_probs)\n",
    "\n",
    "# vote for the majority\n",
    "rf_predictions = vote_majority(prediction_trees)\n",
    "# calculate the accuracy\n",
    "print(f\"Testing accuracy: {accuracy_score(test_labels, rf_predictions):.4f}\")\n",
    "# Calculate the average probability for each class across all trees\n",
    "average_probs = np.mean(prediction_trees_probs, axis=0)\n",
    "# Calculate the average confidence of the predictions\n",
    "average_confidences = np.max(average_probs, axis=1)\n",
    "average_confidence = np.mean(average_confidences)\n",
    "print(f\"Average confidence of predictions: {average_confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.1333\n",
      "Average confidence of predictions: 0.1234\n"
     ]
    }
   ],
   "source": [
    "# do the same analysis using XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize a list to store the XGBoost models\n",
    "xgboost_models = []\n",
    "prediction_xgboost = []\n",
    "prediction_xgboost_probs = []\n",
    "\n",
    "for i in range(loop_num):\n",
    "    # Create a new XGBoost model\n",
    "    xgboost = XGBClassifier(n_estimators=loop_num, random_state=i, learning_rate=0.1)\n",
    "    # Fit the XGBoost model on the latent representations\n",
    "    training_array = train_zs[i]\n",
    "    xgboost.fit(training_array, train_labels)\n",
    "    # Append the XGBoost model to the list\n",
    "    xgboost_models.append(xgboost)\n",
    "\n",
    "    # Predict the test labels using the XGBoost model\n",
    "    testing_array = test_zs[i]\n",
    "    prediction_xgb = xgboost.predict(testing_array)\n",
    "    prediction_xgboost.append(prediction_xgb)\n",
    "\n",
    "    # Predict the test labels using the XGBoost model and calculate the probabilities\n",
    "    prediction_xgb_probs = xgboost.predict_proba(testing_array)\n",
    "    prediction_xgboost_probs.append(prediction_xgb_probs)\n",
    "\n",
    "prediction_xgboost = np.array(prediction_xgboost)\n",
    "prediction_xgboost_probs = np.array(prediction_xgboost_probs)\n",
    "\n",
    "# vote for the majority\n",
    "xgboost_predictions = vote_majority(prediction_xgboost)\n",
    "# calculate the accuracy\n",
    "print(f\"Testing accuracy: {accuracy_score(test_labels, xgboost_predictions):.4f}\")\n",
    "# Calculate the average probability for each class across all trees\n",
    "average_probs = np.mean(prediction_xgboost_probs, axis=0)\n",
    "# Calculate the average confidence of the predictions\n",
    "average_confidences = np.max(average_probs, axis=1)\n",
    "average_confidence = np.mean(average_confidences)\n",
    "print(f\"Average confidence of predictions: {average_confidence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
