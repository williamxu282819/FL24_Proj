How can crowd wisdom be effectively realized, especially when the crowd is overconfident? Representing such crowds with analogous ensembles of many convolutional neural networks (CNNs), I explore different methods of aggregating their prediction outcomes to improve their confidence calibration. I propose a novel crowd aggregation approach: re-learning a combined set of high-dimensional neural representations with independent, low-dimensional decision models, such as random forests. Then, I validate the approach on MNIST and CIFAR-10 datasets across various ensemble sizes, training samples and positive-evidence (PE) conditions simulated through different contrast and noise manipulations. The experiment results demonstrate superior confidence calibration across all scenarios, indicating that the crowd wisdom of neural networks may be more latent than explicit.
